"""
G√©n√©rateur de r√©ponses via Ollama LLM
"""
import requests
import json
import random

class OllamaGenerator:
    def __init__(self, model="llama2", base_url="http://localhost:11434"):
        """
        Initialise le g√©n√©rateur Ollama
        
        Args:
            model: Nom du mod√®le Ollama (llama2, mistral, etc.)
            base_url: URL du serveur Ollama
        """
        self.model = model
        self.base_url = base_url
        self.is_available = self._check_availability()
        
    def _check_availability(self):
        """V√©rifie si Ollama est disponible"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=2)
            return response.status_code == 200
        except:
            return False
    
    def build_prompt(self, emotion, mood_state, user_message=""):
        """
        Construit un prompt contextualis√© bas√© sur l'√©motion d√©tect√©e
        
        Args:
            emotion: √âmotion d√©tect√©e (happy, sad, angry, etc.)
            mood_state: √âtat d'humeur (UP, DOWN, NEUTRAL)
            user_message: Message de l'utilisateur (optionnel)
        
        Returns:
            str: Prompt format√© pour Ollama
        """
        emotion_fr = {
            'happy': 'heureux/heureuse',
            'sad': 'triste',
            'angry': 'en col√®re',
            'fear': 'anxieux/anxieuse',
            'surprise': 'surpris(e)',
            'disgust': 'd√©go√ªt√©(e)',
            'neutral': 'neutre/calme'
        }
        
        emotion_text = emotion_fr.get(emotion, emotion)
        
        system_context = f"""Tu es un assistant empathique et bienveillant. 
L'utilisateur semble {emotion_text} (√©tat: {mood_state}).
G√©n√®re une r√©ponse courte (2-3 phrases maximum) en fran√ßais qui:
- Est empathique et adapt√©e √† son √©tat √©motionnel
- L'encourage ou le r√©conforte selon son humeur
- Reste naturelle et humaine"""

        if user_message:
            prompt = f"{system_context}\n\nMessage de l'utilisateur: \"{user_message}\"\n\nR√©ponds de mani√®re empathique:"
        else:
            prompt = f"{system_context}\n\nG√©n√®re une phrase d'accueil empathique:"
        
        return prompt
    
    def generate_response(self, emotion, mood_state, user_message=""):
        """
        G√©n√®re une r√©ponse via Ollama
        
        Args:
            emotion: √âmotion d√©tect√©e
            mood_state: √âtat d'humeur (UP/DOWN/NEUTRAL)
            user_message: Message utilisateur (contexte)
        
        Returns:
            str: R√©ponse g√©n√©r√©e
        """
        # Si Ollama n'est pas disponible, utiliser fallback
        if not self.is_available:
            print("‚ö†Ô∏è Ollama non disponible, utilisation du fallback")
            return self._fallback_response(mood_state)
        
        prompt = self.build_prompt(emotion, mood_state, user_message)
        
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 150
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get('response', '').strip()
                
                # Nettoyer la r√©ponse
                if generated_text:
                    return generated_text
                else:
                    return self._fallback_response(mood_state)
            else:
                print(f"‚ùå Erreur Ollama: {response.status_code}")
                return self._fallback_response(mood_state)
                
        except requests.exceptions.Timeout:
            print("‚è±Ô∏è Timeout Ollama")
            return self._fallback_response(mood_state)
        except Exception as e:
            print(f"‚ùå Erreur Ollama: {e}")
            return self._fallback_response(mood_state)
    
    def _fallback_response(self, mood_state):
        """R√©ponses de secours si Ollama ne r√©pond pas"""
        fallbacks = {
            "DOWN": [
                "Je suis l√† pour t'√©couter. Comment puis-je t'aider ? üíô",
                "Je vois que tu ne vas pas au top. Veux-tu en parler ? üòî",
                "Prends ton temps, je suis l√† pour t'accompagner. ü§ó"
            ],
            "UP": [
                "C'est super de te voir de bonne humeur ! üòä",
                "Quelle belle √©nergie positive ! Continue comme √ßa ! ‚ú®",
                "Tu rayonnes aujourd'hui ! Raconte-moi ce qui te rend heureux(se) ! üåü"
            ],
            "NEUTRAL": [
                "Comment puis-je t'aider aujourd'hui ? üòä",
                "Je suis l√† pour discuter si tu en as envie. üí¨",
                "Tu as l'air serein(e). Comment te sens-tu ? üôÇ"
            ]
        }
        
        responses = fallbacks.get(mood_state, fallbacks["NEUTRAL"])
        return random.choice(responses)
    
    def test_connection(self):
        """Test la connexion √† Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                print(f"‚úÖ Ollama connect√© ! Mod√®les disponibles:")
                for model in models:
                    print(f"  - {model['name']}")
                return True
            else:
                print(f"‚ùå Ollama non accessible (status: {response.status_code})")
                return False
        except Exception as e:
            print(f"‚ùå Impossible de se connecter √† Ollama: {e}")
            return False
